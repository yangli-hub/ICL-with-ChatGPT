# ICL with ChatGPT
#### Author: Li YANG, yang0666@e.ntu.edu.sg
#### The Corresponding Paper: 
##### An empirical study of Multimodal Entity-Based Sentiment Analysis with ChatGPT: Improving in-context learning via entity-aware contrastive learning
##### https://www.sciencedirect.com/science/article/abs/pii/S0306457324000840

##### The framework of the ICL with ChatGPT: 
![alt text]![method](https://github.com/yangli-hub/ICL-with-ChatGPT/assets/70850281/51aab505-0460-463c-b306-67ce4335c3e9)

## 1. Data Set
- The original full set of datasets, namely Twitter-15 and Twitter-17, can be saved to the designated folder "./org_data". 
   
- The datasets we utilize are sourced from https://github.com/YangXiaocui1215/GMP, which are extracted from subsets of Twitter-15 and Twitter-17. To store these datasets, we save them into the folder named "./subset_data".

- The corresponding images can be downloaded via the following link:
https://drive.google.com/file/d/1PpvvncnQkgDNeBMKVgG2zFYuRhbL873g/view


## 2. Utilize BLIP to generate Image caption, VQA-entity, and VQA-sentiment for all the images.
- This module delves into the Visual Input Construction Section in the paper, leveraging the BLIP model for its operations. It utilizes image captioning to generate descriptions for each image, alongside extracting entity and sentiment information through VQA.

- BLIP models via the Hugging Face link provided here: https://huggingface.co/collections/Salesforce/blip-models-65242f40f1491fbf6a9e9472.
   
- The generated outputs can be conveniently saved to the folders named "./output_vqa" and "./output_caption". 

## 3. Positive and Negative Instance Construction
- This module pertains to the section of Positive and Negative Instance Construction within the paper. 

- It is implemented in the file named "pos_neg_instance.py". 

- The output generated by this module is stored in the directory "./output_pos_neg_instances".

## 4. Train the SimCSE model
- This module is designed for the Section Entity-aware Contrastive Learning as described in the paper. Its input comprises the outputs of the preceding step labeled "Generate Positive and Negative Instances," which consist of pairs of positive and negative instances.
  
- The SimCSE backbone is available for download from the following GitHub repository: https://github.com/princeton-nlp/SimCSE. Instructions detailing the environment setup and steps necessary to train the SimCSE model are provided on the repository's page. We utilize the "princeton-nlp/sup-simcse-roberta-large" version of the model, although alternative backbone models may also be employed depending on the specific task requirements.

- It is imperative to save the trained model once the training process is completed.

## 5. Utilize the trained Contrastive Learning model to generate a selection of N similar samples, facilitating the execution of few-shot learning within ChatGPT.
- This module is specifically designed for the section of Retrieving Similar Samples Using the Entity-aware Contrastive Learning Model outlined in the paper.
   
- Here, we utilize the trained contrastive learning model from the previous step to compute the similarities between test samples and samples in the training set. Following this computation, top N similar samples are select based on highest cosine similarity. These selected samples serve as demonstrations for the In-context learning process.
   
- The corresponding code for this module can be found in "prompt_generation.py".

- The output of this module generates N selected samples for each test sample, which will serve as few-shot learning samples in the subsequent step.

## 6. Leveraging the ChatGPT API for Few-Shot Learning
 To initiate the process of few-shot learning utilizing the ChatGPT API, one can refer to the comprehensive guidelines provided in the official documentation on ChatGPT usage by OpenAI.

## Acknowledgements
 Using the datasets of Twitter-15 and Twitter-17 means you have read and accepted the copyrights set by Twitter and dataset providers.

## Citation Information:

Yang, L., Wang, Z., Li, Z., Na, J. C., & Yu, J. (2024). An empirical study of Multimodal Entity-Based Sentiment Analysis with ChatGPT: Improving in-context learning via entity-aware contrastive learning. Information Processing & Management, 61(4), 103724.
